---
title: "Roadmap"
format: 
  html:
    page-layout: full
---

```{python}
#| label: roadmap
#| echo: false
#| output: asis

import vote
    
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from itables import init_notebook_mode, show

init_notebook_mode(all_interactive=False)

# Load voting results
results = vote.run_election('./roadmap/votes', 1, './roadmap/tasks.csv')
avg_table = results['avg_table']

# Load all tasks (preserve Effort and other columns)
tasks_df = pd.read_csv('./roadmap/tasks.csv')
if 'Priority' not in tasks_df.columns:
    tasks_df['Priority'] = 0
# Merge in Priority from avg_table (by Task, left join)
priority_df = avg_table[['Task', 'Priority']]
df = tasks_df.merge(priority_df, on='Task', how='left', suffixes=('', '_voted'))
# Use voted Priority if available, else fallback to original
df['Priority'] = df['Priority_voted'].fillna(df['Priority'])
df.drop(columns=['Priority_voted'], inplace=True)

# Ensure Effort is numeric, fill missing with 0, and filter out negative Effort
df['Effort'] = pd.to_numeric(df['Effort'], errors='coerce').fillna(0)
df = df[df['Effort'] >= 0]

# Filter out completed and archived tasks (do not show in chart)
if 'Completed_on' in df.columns:
    df = df[df['Completed_on'].isna()]
if 'Archived_on' in df.columns:
    df = df[df['Archived_on'].isna() | (df['Archived_on'] == '')]
df = df.drop(columns=['Completed_on', 'Archived_on'], errors='ignore')

# Reorder columns
if 'Created_on' in df.columns:
    df = df[['Category', 'Task', 'Priority', 'Effort', 'Notes', 'Created_on']]
else:
    df = df[['Category', 'Task', 'Priority', 'Effort', 'Notes']]

# Use Latency (age in days since Created_on) as x-axis, Effort as marker size
import numpy as np
from datetime import datetime
min_size = 60  # Increased from 30 to 60
max_size = 150  # Increased from 100 to 150
def compute_latency(row):
    try:
        if 'Created_on' not in row or pd.isna(row['Created_on']):
            return np.nan
        created = pd.to_datetime(row['Created_on'], errors='coerce')
        if pd.isna(created):
            return np.nan
        return (datetime.now() - created).days
    except Exception:
        return np.nan
df['Latency'] = df.apply(compute_latency, axis=1)

# Calculate raw vote count for each task
votes_dict = vote.get_votes('./roadmap/votes')
vote_counts = {}
for voter, ballot in votes_dict.items():
    for task, vote_val in ballot.items():
        vote_counts[task] = vote_counts.get(task, 0) + vote_val

total_votes = sum(vote_counts.values()) if vote_counts else 1
# Add normalized vote count to dataframe
# If a task has no votes, set to 0
df['RawVoteCount'] = df['Task'].map(lambda t: vote_counts.get(t, 0))
df['NormVoteCount'] = df['RawVoteCount'] / total_votes

# Compute Pareto frontier: maximize Priority, minimize Effort
pareto_tasks = []
for idx, row in df.iterrows():
    dominated = df[
        ((df['Priority'] > row['Priority']) & (df['Effort'] <= row['Effort'])) |
        ((df['Priority'] >= row['Priority']) & (df['Effort'] < row['Effort']))
    ]
    if dominated.empty:
        pareto_tasks.append(row['Task'])
# Highlight Pareto-optimal tasks
df['Pareto'] = df['Task'].isin(pareto_tasks)
df['Task_Display'] = df.apply(lambda r: f"{r['Task']} ðŸš¨" if r['Pareto'] else r['Task'], axis=1)

# Use Latency for marker size, fill missing with min_size
latency_sizes = df['Latency'].fillna(min_size)
fig = px.scatter_3d(
    df,
    x='RawVoteCount',
    y='Effort',
    z='Priority',
    color='Pareto',
    size=latency_sizes,
    hover_data=['Task_Display', 'Category', 'Notes', 'Effort', 'Latency', 'Created_on', 'RawVoteCount', 'NormVoteCount'],
    title='Feature Roadmap (3D): Vote Count vs Effort vs Priority (Pareto Frontier, Latency-Scaled)',
    labels={'RawVoteCount': 'Raw Vote Count', 'Effort': 'Effort (1-100 Scale)', 'Priority': 'Priority (Higher = More Votes)', 'Created_on': 'Created', 'Latency': 'Age (days)', 'NormVoteCount': 'Normalized Vote Count'}
)
fig.show()

# Convex hull visualization for Pareto-optimal tasks using scipy
pareto_df = df[df['Pareto']]
if len(pareto_df) >= 3:
    try:
        from scipy.spatial import ConvexHull
        import numpy as np
        points = pareto_df[['Effort', 'Priority']].values
        hull = ConvexHull(points)
        hull_points = points[hull.vertices]
        print('Pareto tasks:')
        print(pareto_df[['Task', 'Effort', 'Priority']])
        print('Convex hull points:')
        print(hull_points)
        hull_trace = go.Scatter(
            x=hull_points[:,0],
            y=hull_points[:,1],
            mode='lines+markers',
            name='Pareto Convex Hull (scipy)',
            line=dict(color='orange', width=3, dash='dash'),
            marker=dict(size=8, color='orange', symbol='diamond')
        )
        fig2d = px.scatter(
            pareto_df,
            x='Effort',
            y='Priority',
            color='Category',
            hover_data=['Task_Display', 'Notes', 'Created_on', 'RawVoteCount', 'NormVoteCount']
        )
        fig2d.add_trace(hull_trace)
        fig2d.update_layout(title='Pareto Tasks Convex Hull (Effort vs Priority, scipy)')
        fig2d.show()
    except ImportError:
        print('scipy is not installed. Convex hull visualization skipped.')

print("""

## Glossary

**Pareto Frontier**: In multi-objective optimization, the Pareto frontier (or Pareto front) is the set of solutions that are not dominated by any other solution. A solution is considered Pareto optimal if no other solution is better in all objectives. In this roadmap, tasks on the Pareto frontier have the highest priority for their level of effort, meaning you cannot improve priority without increasing effort, or reduce effort without lowering priority.

**Latency**: The age of a task, measured in days since it was created. Used to visualize how long a task has been in the roadmap.

**Effort**: An estimated measure of the work required to complete a task, typically on a scale from 1 to 100.

**Priority**: A score representing the importance or urgency of a task, often based on votes or other ranking methods.

**Raw Vote Count**: The total number of votes a task has received from all voters.

**Normalized Vote Count**: The raw vote count divided by the total number of votes, used to compare tasks on a common scale.
""")

print("""
## Archived Tasks

Archived tasks

<details>
<summary>Show archived tasks</summary>
""")

archived_tasks_df = vote.get_archived_tasks_table('./roadmap/tasks.csv')
archived_tasks_df['Notes'] = archived_tasks_df['Notes'].fillna("")
archived_tasks_df.reset_index(drop=True, inplace=True)

show(archived_tasks_df)

print("""
</details>
""")

print("""

## Raw Votes

<details>
<summary>Show raw votes table</summary>

Raw votes count

""")

votes_dict = vote.get_votes('./roadmap/votes')
votes_df = pd.DataFrame(votes_dict)
votes_df = votes_df.fillna(0)
show(votes_df, paging=False)

print("""
</details>
""")
```
